{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1eb33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "num_merges = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5198548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    ids = list of inputs\n",
    "    pair = target pair to replace\n",
    "    idx = number to replace with\n",
    "    \"\"\"\n",
    "\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 # skip over two indices\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "    # print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9df0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_stats(bytes):\n",
    "    # loops through all existing byte pairs and counts them\n",
    "    counts = {}\n",
    "    for pair in zip(bytes, bytes[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aecc8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { idx: bytes([idx]) for idx in range(256)} # create a dict that maps ints to byte versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf88938",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, ids)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {}\n",
    "\n",
    "for i in range(num_merges):\n",
    "        stats = get_stats(tokens) # get counts of each byte pair\n",
    "        pair = max(stats, key=stats.get) # find largest pair by value (number of occurences)\n",
    "        idx = 256 + i # create new token value\n",
    "        tokens = merge(tokens, pair, idx) # replace all instances of the pair with the new token\n",
    "        merges[pair] = idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27b386ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1] # iterate through merges, setting the newly minted tokens' values to be the bytes of the merged pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19ddd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode(ids): # ids is a series of integers that represent tokens. 1-256 are the og tokens, 256+ are newly created ones\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids) # basically just create a bytestream \n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\") # errors=replace to make sure invalid bytes get converted properly\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d05c101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruh\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given string, return list of integers (tokens )\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) # take all the pairs from stats, look in merges dict, find the lowest merged token number\n",
    "        # ie 257 not 258, and return the pair that corresponds to that merge\n",
    "        if pair not in merges:\n",
    "            break # nothign else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(decode(encode(\"bruh\")))\n",
    "\n",
    "# so we basically trained a tokenizer\n",
    "# the weights manifest in terms of:\n",
    "# - the vocab contains the mappings of the token id to the byte(s) that form that token\n",
    "# - the merges represent how the bytes were merged to form the token ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "# regex goes from left to right on the matching string ^ and tries to match. in the first example in the vid, \"Hello World\", the first thing that matches is\n",
    "# the  ?\\p{L}+, which means optional space, p{L}+ which means any number of letters from any language. in \"Hello World\", this will create two matches, as the\n",
    "# second space does not count as a letter\n",
    "\n",
    "# the section that is ^\\s... means, match something that is not a letter, number or space, so its matching for punctuation, like ?!!?!\n",
    "# the section after that is a space lookahead, trying to match !\\S, meaning that if theres multiple white spaces, the regex will only match up until the second to last space\n",
    "# this is important because space + word is a very common token, so we want to make more of those tokens, \n",
    "\n",
    "# one interesting thing to note is that the gpt2 authors note that they didn't include regex.IGNORECASE, which basically nullifies a lot of the matching\n",
    "# eg: WORD'S will match as WORD, ', S instead of WORD, 'S\n",
    "\n",
    "# also, idk how this works with nonenglish languages.\n",
    "\n",
    "# basically what the gpt tokenizer does is it first splits the text into texts of text, and then does the tokenization/merging separately per text\n",
    "# afterward, everything is concatenated. not too sure what concatenated means, im assuming it means like continue merging after the individual merges happen\n",
    "print(re.findall(gpt2pat, \"Hello123 World's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc05cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "# testing tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# gpt 2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})) #idk why this no work :(\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# one interesting change in the gpt 4 tokenizer is that they do matching for 3 numbers max, which is to prevent very long numbers to become tokens\n",
    "# another interesting thing that karpathy notes is that for gpt 2, there is 256 byte tokens, 50k merges, and 1 special token, in which that special one is <|endoftext|>\n",
    "\n",
    "# karpathy notes that in the fine tuned gpt models, there are actually a bunch more tokens like <|im_start|> (imaginary monologue) start for the chat gpt interface\n",
    "# fill in the middle (FIM) is another special token that they have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c75f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
